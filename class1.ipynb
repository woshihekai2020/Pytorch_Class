{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0194e-38, 8.4490e-39, 1.0469e-38],\n",
      "        [9.3674e-39, 9.9184e-39, 8.7245e-39],\n",
      "        [9.2755e-39, 8.9082e-39, 9.9184e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 1.0653e-38],\n",
      "        [1.0469e-38, 4.2246e-39, 1.0378e-38]])\n"
     ]
    }
   ],
   "source": [
    "#创建tensor\n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1788, 0.7309, 0.7310],\n",
      "        [0.7712, 0.5126, 0.3541],\n",
      "        [0.4170, 0.2819, 0.0045],\n",
      "        [0.5601, 0.1818, 0.2677],\n",
      "        [0.3670, 0.9997, 0.4974]])\n"
     ]
    }
   ],
   "source": [
    "#创建随机tensor\n",
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建long型tensor\n",
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "print(x)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3)\n",
    "x.dtype\n",
    "#与上一cell的x拥有相同的dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#直接创建，叶子节点\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x000001A54067C8D0>\n"
     ]
    }
   ],
   "source": [
    "#加法创建y，拥有grad_fn\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "print(x.is_leaf,y.is_leaf)\n",
    "#x是叶子节点，y不是叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#来点复杂节点\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出z与z的平均值，grad_fn有响应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x000001A57DA02AC8>\n"
     ]
    }
   ],
   "source": [
    "#requires_grad属性改变\n",
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print(a.requires_grad)#false\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)#true\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:149: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:115.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    }
   ],
   "source": [
    "#out反向求导\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "#out对x的导数\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n"
     ]
    }
   ],
   "source": [
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "#注意grad是累加的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "out3 = x.sum()\n",
    "x.grad.data.zero_() #x梯度清零\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9180,  1.8064, -0.1352],\n",
      "        [-0.3963,  0.3627, -0.1040],\n",
      "        [-1.0239,  1.0451, -1.3425],\n",
      "        [ 2.4770, -1.7507,  0.1319],\n",
      "        [-0.1877, -1.1436,  1.0243]])\n",
      "tensor([[ 0.9180,  1.8064, -0.1352, -0.3963,  0.3627],\n",
      "        [-0.1040, -1.0239,  1.0451, -1.3425,  2.4770],\n",
      "        [-1.7507,  0.1319, -0.1877, -1.1436,  1.0243]])\n",
      "tensor([ 0.9180,  1.8064, -0.1352, -0.3963,  0.3627, -0.1040, -1.0239,  1.0451,\n",
      "        -1.3425,  2.4770, -1.7507,  0.1319, -0.1877, -1.1436,  1.0243])\n",
      "tensor([[ 0.9180,  1.8064, -0.1352, -0.3963,  0.3627],\n",
      "        [-0.1040, -1.0239,  1.0451, -1.3425,  2.4770],\n",
      "        [-1.7507,  0.1319, -0.1877, -1.1436,  1.0243]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,3)\n",
    "print(x)#create tensor\n",
    "y = x.view(3,5)#change the shape\n",
    "print(y)\n",
    "z = x.view(-1) #chage the shape\n",
    "print(z)\n",
    "p = x.view(-1,5)#change the shape\n",
    "print(p)\n",
    "#attention:共享内存，一个改变以上都会改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9180,  2.8064,  0.8648],\n",
      "        [ 0.6037,  1.3627,  0.8960],\n",
      "        [-0.0239,  2.0451, -0.3425],\n",
      "        [ 3.4770, -0.7507,  1.1319],\n",
      "        [ 0.8123, -0.1436,  2.0243]])\n",
      "tensor([[ 1.9180,  2.8064,  0.8648,  0.6037,  1.3627],\n",
      "        [ 0.8960, -0.0239,  2.0451, -0.3425,  3.4770],\n",
      "        [-0.7507,  1.1319,  0.8123, -0.1436,  2.0243]])\n"
     ]
    }
   ],
   "source": [
    "y += 1\n",
    "print(x)\n",
    "print(y)\n",
    "#共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9180,  2.8064,  0.8648],\n",
      "        [ 0.6037,  1.3627,  0.8960],\n",
      "        [-0.0239,  2.0451, -0.3425],\n",
      "        [ 3.4770, -0.7507,  1.1319],\n",
      "        [ 0.8123, -0.1436,  2.0243]])\n",
      "tensor([2.9180, 3.8064, 1.8648, 1.6037, 2.3627, 1.8960, 0.9761, 3.0451, 0.6575,\n",
      "        4.4770, 0.2493, 2.1319, 1.8123, 0.8564, 3.0243])\n"
     ]
    }
   ],
   "source": [
    "#那么不共享内存应该怎么做\n",
    "y = x.clone().view(15)\n",
    "y += 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1800])\n",
      "0.17995275557041168\n"
     ]
    }
   ],
   "source": [
    "#item() tensor->python number\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 其他线性函数 <br />\n",
    "trace -> 对角线之和（矩阵的秩）<br />\n",
    "diag  -> 对角线元素<br />\n",
    "triu/tril -> 上三角/下三角<br />\n",
    "mm/bmm -> 矩阵乘法，batch乘法运算<br />\n",
    "addmm/addbmm/addmv/addmv/addr/badbmm -> 矩阵运算<br />\n",
    "t -> 转置<br />\n",
    "dot/class -> 内积/外积<br />\n",
    "inverse -> 求逆矩阵<br />\n",
    "svd -> 奇异值分解<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4]], dtype=torch.int32)\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "tensor([[2, 3, 4, 5],\n",
      "        [2, 3, 4, 5],\n",
      "        [2, 3, 4, 5],\n",
      "        [2, 3, 4, 5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#何为广播：将数据按一定规律扩增,有元素被复制\n",
    "x = torch.arange(1,5,dtype = torch.int).view(-1,4)\n",
    "y = torch.ones(4,dtype=torch.int).view(4,1)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([4, 1])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]], dtype=torch.int32)\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(y.size())\n",
    "print(x*y)\n",
    "print(torch.mm(y,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内存开销"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_before: 1808853909440\n",
      "id 1808853909008\n"
     ]
    }
   ],
   "source": [
    "#内存开销\n",
    "#y = x + y 为新的y开辟了新内存\n",
    "x = torch.tensor([3,4])\n",
    "y = torch.tensor([5,6])\n",
    "id_before = id(y)\n",
    "y = x + y\n",
    "print(\"id_before:\",id_before)\n",
    "print(\"id\",id(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_before: 1808862866024\n",
      "id 1808862866024\n",
      "tensor([3, 4])\n",
      "tensor([ 8, 10])\n"
     ]
    }
   ],
   "source": [
    "#那么如何覆盖y原来的内存呢\n",
    "#方法一\n",
    "x = torch.tensor([3,4])\n",
    "y = torch.tensor([5,6])\n",
    "id_before = id(y)\n",
    "y.add_(x)\n",
    "print(\"id_before:\",id_before)\n",
    "print(\"id\",id(y))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_before: 1808856324928\n",
      "id 1808856324928\n",
      "tensor([3, 4])\n",
      "tensor([ 8, 10])\n"
     ]
    }
   ],
   "source": [
    "#那么如何覆盖y原来的内存呢\n",
    "#方法\n",
    "x = torch.tensor([3,4])\n",
    "y = torch.tensor([5,6])\n",
    "id_before = id(y)\n",
    "torch.add(x,y,out=y)#等效于y+=x,y.add_(x)\n",
    "print(\"id_before:\",id_before)\n",
    "print(\"id\",id(y))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor 与 numpy 相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#tensor->numpy\n",
    "a = torch.ones(4,5)\n",
    "b = a.numpy()\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]]) [[2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]]\n",
      "tensor([[3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3.]]) [[3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]\n",
      " [3. 3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "a += 1\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍然共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#numpy->tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a,b)\n",
    "a+=1\n",
    "print(a,b)\n",
    "b+=1\n",
    "print(a,b)\n",
    "#共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 5. 5. 5. 5.] tensor([4., 4., 4., 4., 4.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#如何不共享内存,创建新tensor->torch.tensor()\n",
    "c = torch.tensor(a)\n",
    "a+=1\n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x,device=device)  #直接创建一个在GPU上的tensor\n",
    "    x = x.to(device)\n",
    "    z = x+y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\",torch.double))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
